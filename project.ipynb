{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d337b455",
   "metadata": {
    "id": "d337b455"
   },
   "source": [
    "# Customer Churn Business Prediction Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5ad547",
   "metadata": {
    "id": "5f5ad547"
   },
   "source": [
    "## I. Exploring the Dataset & Workflow Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "b21a1b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367c9da8",
   "metadata": {
    "id": "367c9da8"
   },
   "source": [
    "### 0. Setting up the seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33f9429",
   "metadata": {
    "id": "b33f9429"
   },
   "source": [
    "Let's set the seed to 42, in order to ensure results are reproducible across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3a4f0e65",
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1769957375337,
     "user": {
      "displayName": "Cristi Turcin",
      "userId": "04700934510619811375"
     },
     "user_tz": -120
    },
    "id": "3a4f0e65"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c3625",
   "metadata": {
    "id": "566c3625"
   },
   "source": [
    "### 1. Printing the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc23cf9d",
   "metadata": {
    "id": "bc23cf9d"
   },
   "source": [
    "First we will open the dataset using pandas in order to visualise the available features. We will read it from the `.csv` file and insert it into the `ccb_dataset` as a data frame. To print the all the available features we will use `.info` function. Also, we can see that in order to view all the available features I have exported them in an `.xlsx` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1a360f62",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2099,
     "status": "ok",
     "timestamp": 1769957403058,
     "user": {
      "displayName": "Cristi Turcin",
      "userId": "04700934510619811375"
     },
     "user_tz": -120
    },
    "id": "1a360f62",
    "outputId": "cd84b7c0-41d3-4641-e4dc-6aa1e9f46e40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 32 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   customer_id             10000 non-null  object \n",
      " 1   gender                  10000 non-null  object \n",
      " 2   age                     10000 non-null  int64  \n",
      " 3   country                 10000 non-null  object \n",
      " 4   city                    10000 non-null  object \n",
      " 5   customer_segment        10000 non-null  object \n",
      " 6   tenure_months           10000 non-null  int64  \n",
      " 7   signup_channel          10000 non-null  object \n",
      " 8   contract_type           10000 non-null  object \n",
      " 9   monthly_logins          10000 non-null  int64  \n",
      " 10  weekly_active_days      10000 non-null  int64  \n",
      " 11  avg_session_time        10000 non-null  float64\n",
      " 12  features_used           10000 non-null  int64  \n",
      " 13  usage_growth_rate       10000 non-null  float64\n",
      " 14  last_login_days_ago     10000 non-null  int64  \n",
      " 15  monthly_fee             10000 non-null  int64  \n",
      " 16  total_revenue           10000 non-null  int64  \n",
      " 17  payment_method          10000 non-null  object \n",
      " 18  payment_failures        10000 non-null  int64  \n",
      " 19  discount_applied        10000 non-null  object \n",
      " 20  price_increase_last_3m  10000 non-null  object \n",
      " 21  support_tickets         10000 non-null  int64  \n",
      " 22  avg_resolution_time     10000 non-null  float64\n",
      " 23  complaint_type          7955 non-null   object \n",
      " 24  csat_score              10000 non-null  float64\n",
      " 25  escalations             10000 non-null  int64  \n",
      " 26  email_open_rate         10000 non-null  float64\n",
      " 27  marketing_click_rate    10000 non-null  float64\n",
      " 28  nps_score               10000 non-null  int64  \n",
      " 29  survey_response         10000 non-null  object \n",
      " 30  referral_count          10000 non-null  int64  \n",
      " 31  churn                   10000 non-null  int64  \n",
      "dtypes: float64(6), int64(14), object(12)\n",
      "memory usage: 2.4+ MB\n",
      "Dataset shape: (10000, 32)\n",
      "churn\n",
      "0    0.8979\n",
      "1    0.1021\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ccb_dataset = pd.read_csv(\n",
    "    # '/content/drive/MyDrive/FML/Project/customer_churn_business_dataset.csv'\n",
    "    './customer_churn_business_dataset.csv'\n",
    ")\n",
    "info_df = pd.DataFrame({\n",
    "    \"Column\": ccb_dataset.columns,\n",
    "    \"Data Type\": ccb_dataset.dtypes,\n",
    "    \"Non-Null Count\": ccb_dataset.notnull().sum()\n",
    "})\n",
    "\n",
    "# info_df.to_excel(\"ccb_columns.xlsx\", sheet_name=\"features\")\n",
    "\n",
    "ccb_dataset.info()\n",
    "print(\"Dataset shape:\", ccb_dataset.shape)\n",
    "print(ccb_dataset['churn'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b168cd",
   "metadata": {
    "id": "a1b168cd"
   },
   "source": [
    "### 2. Defining the problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ab237c",
   "metadata": {
    "id": "65ab237c"
   },
   "source": [
    "This is a __classification problem__ which can be solved using __supervised learning__. Our task is to predict whether a certain customer will churn, meaning he will unsubscribe from our service. Our target variable is `churn`, which has two possible values:\n",
    "- __0__ → \"No\" (customer did not churn)  \n",
    "- __1__ → \"Yes\" (customer churned)\n",
    "\n",
    "Each entry in our dataset represents a certain user, along with various characteristics. Some features are likely more important for predicting churn than others. For example, features such as `avg_session_time`, `last_login_days_ago`, `csat_score`, `nps_score` and `survey_response` may have a strong influence on whether a customer churns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ef3534",
   "metadata": {
    "id": "81ef3534"
   },
   "source": [
    "### 3. Workflow Plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b3c397",
   "metadata": {
    "id": "e5b3c397"
   },
   "source": [
    "**Step 1: Data Cleaning**  \n",
    "This step will consist of removing irrelevant or uninformative features, such as identifiers or columns that do not contribute to predicting customer churn.\n",
    "\n",
    "**Step 2: Preprocessing**  \n",
    "Categorical features will be encoded using appropriate encoding techniques. Numeric features will be scaled to ensure they are suitable for models that rely on distance-based optimization.\n",
    "\n",
    "**Step 3: Training Strategies**  \n",
    "Two training datasets will be created:\n",
    "- One dataset using all available features\n",
    "- One dataset obtained after applying Principal Component Analysis (PCA) to reduce dimensionality while retaining most of the variance\n",
    "\n",
    "The performance of both approaches will be compared.\n",
    "\n",
    "**Step 4: Modeling**  \n",
    "Support Vector Machines (SVM), Logistic Regression and Decision Trees will be used to predict the target variable `churn`. Model performance will be evaluated and compared using appropriate classification metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca9516",
   "metadata": {
    "id": "8cca9516"
   },
   "source": [
    "## II. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7590a8d8",
   "metadata": {},
   "source": [
    "### 0. Datasets\n",
    "\n",
    "For this project we are proposing four datasets:\n",
    "- the first will have a __high__ amount of features. We will not care about the qualitative relevance of them.\n",
    "- the second will have only features that are considered relevant to predicting `churn`\n",
    "- this one will have only __two features__ decided by us using a __qualitative__ analysis\n",
    "- last but not least we will have a dataset containting __three__ features based on the __amount of variance__ they have to offer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190e4dc8",
   "metadata": {},
   "source": [
    "### 1. Many Features Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a233a2",
   "metadata": {
    "id": "e9a233a2"
   },
   "source": [
    "We will keep most features except of the following ones:\n",
    "\n",
    "**Identifiers**\n",
    "- `customer_id`\n",
    "\n",
    "**Marketing & support**\n",
    "- `email_open_rate`\n",
    "- `marketing_click_rate`\n",
    "- `complaint_type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "36b78162",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1769957406309,
     "user": {
      "displayName": "Cristi Turcin",
      "userId": "04700934510619811375"
     },
     "user_tz": -120
    },
    "id": "36b78162",
    "outputId": "dce574aa-45e2-4736-ad02-ccc4e51cef61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender  age     country      city customer_segment  tenure_months  \\\n",
      "0    Male   68  Bangladesh    London              SME             22   \n",
      "1  Female   57      Canada    Sydney       Individual              9   \n",
      "2    Male   24     Germany  New York              SME             58   \n",
      "3    Male   49   Australia     Dhaka       Individual             19   \n",
      "4    Male   65  Bangladesh     Delhi       Individual             52   \n",
      "\n",
      "  signup_channel contract_type  monthly_logins  weekly_active_days  ...  \\\n",
      "0            Web       Monthly              26                   7  ...   \n",
      "1         Mobile       Monthly               7                   5  ...   \n",
      "2            Web        Yearly              19                   5  ...   \n",
      "3         Mobile        Yearly              34                   7  ...   \n",
      "4            Web       Monthly              20                   6  ...   \n",
      "\n",
      "   discount_applied  price_increase_last_3m  support_tickets  \\\n",
      "0               Yes                      No                4   \n",
      "1                No                     Yes                1   \n",
      "2                No                      No                1   \n",
      "3               Yes                      No                3   \n",
      "4                No                      No                0   \n",
      "\n",
      "   avg_resolution_time  csat_score  escalations nps_score  survey_response  \\\n",
      "0            13.354360         4.0            0        27        Satisfied   \n",
      "1            25.140088         2.0            0       -19          Neutral   \n",
      "2            27.572928         3.0            0        80          Neutral   \n",
      "3            26.420822         5.0            1       100          Neutral   \n",
      "4            26.674579         4.0            0        21      Unsatisfied   \n",
      "\n",
      "  referral_count churn  \n",
      "0              1     0  \n",
      "1              2     1  \n",
      "2              1     0  \n",
      "3              0     0  \n",
      "4              1     0  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "many_feat_dataset = ccb_dataset.drop(\n",
    "    columns=[\n",
    "        'customer_id',\n",
    "        'email_open_rate',\n",
    "        'marketing_click_rate',\n",
    "        'complaint_type'\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(many_feat_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ae344",
   "metadata": {},
   "source": [
    "### 2. Relevant Features Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a90bd0",
   "metadata": {},
   "source": [
    "The following features will be considered irrelevant (based on domain knowledge) for predicting `churn`:\n",
    "\n",
    "**Identifiers**\n",
    "- `customer_id`\n",
    "\n",
    "**Demographics**\n",
    "- `gender`\n",
    "- `age`\n",
    "- `country`\n",
    "- `city`\n",
    "\n",
    "**Customer profile**\n",
    "- `customer_segment`\n",
    "- `signup_channel`\n",
    "- `contract_type`\n",
    "\n",
    "**Usage & engagement**\n",
    "- `avg_session_time`\n",
    "- `features_used`\n",
    "- `last_login_days_ago`\n",
    "\n",
    "**Pricing & billing**\n",
    "- `monthly_fee`\n",
    "- `total_revenue`\n",
    "- `payment_method`\n",
    "- `discount_applied`\n",
    "- `price_increase_last_3m`\n",
    "\n",
    "**Marketing & support**\n",
    "- `email_open_rate`\n",
    "- `marketing_click_rate`\n",
    "- `complaint_type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0bd81760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   monthly_logins  weekly_active_days  usage_growth_rate  payment_failures  \\\n",
      "0              26                   7               0.06                 1   \n",
      "1               7                   5              -0.28                 1   \n",
      "2              19                   5               0.13                 2   \n",
      "3              34                   7              -0.17                 0   \n",
      "4              20                   6              -0.16                 0   \n",
      "\n",
      "   support_tickets  avg_resolution_time  csat_score  escalations  nps_score  \\\n",
      "0                4            13.354360         4.0            0         27   \n",
      "1                1            25.140088         2.0            0        -19   \n",
      "2                1            27.572928         3.0            0         80   \n",
      "3                3            26.420822         5.0            1        100   \n",
      "4                0            26.674579         4.0            0         21   \n",
      "\n",
      "  survey_response  referral_count  churn  \n",
      "0       Satisfied               1      0  \n",
      "1         Neutral               2      1  \n",
      "2         Neutral               1      0  \n",
      "3         Neutral               0      0  \n",
      "4     Unsatisfied               1      0  \n"
     ]
    }
   ],
   "source": [
    "rel_feat_dataset = ccb_dataset.drop(columns=['customer_id', 'gender', 'age', 'country', 'city', 'customer_segment', 'tenure_months', 'signup_channel', 'contract_type', 'avg_session_time', 'features_used', 'last_login_days_ago', 'monthly_fee', 'total_revenue', 'payment_method', 'discount_applied', 'price_increase_last_3m', 'complaint_type', 'email_open_rate', 'marketing_click_rate'])\n",
    "\n",
    "print(rel_feat_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b4b8fe",
   "metadata": {},
   "source": [
    "### 3. High Quality Features dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf86b6c4",
   "metadata": {},
   "source": [
    "In order to predict `churn` we believe `csat_score` and `nps_score` are by far the features of the highest importance in the entire dataset.\n",
    "\n",
    "CSAT Score stands for __Customer Satisfaction Score__. It typically ranges from __1__ to __5__, 5 being the highest score. Enough said.\n",
    "\n",
    "NPS Score stands for __Net Promoter Score__. It shows how likely a customer is to recommend a product to someone else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "776a839a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   csat_score  nps_score  churn\n",
      "0         4.0         27      0\n",
      "1         2.0        -19      1\n",
      "2         3.0         80      0\n",
      "3         5.0        100      0\n",
      "4         4.0         21      0\n"
     ]
    }
   ],
   "source": [
    "hq_feat_dataset = ccb_dataset[['csat_score', 'nps_score', 'churn']].copy()\n",
    "\n",
    "print(hq_feat_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf7c73",
   "metadata": {},
   "source": [
    "The reduced datacet will be created later in this project using the __PCA__ algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b94959",
   "metadata": {
    "id": "22b94959"
   },
   "source": [
    "## III. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c3cd23",
   "metadata": {
    "id": "e1c3cd23"
   },
   "source": [
    "### 1. Encoding\n",
    "\n",
    "We can observe that several features in the dataset are categorical and represented as strings. Since machine learning models such as Logistic Regression and Support Vector Machines require numerical input, these categorical variables must be encoded.\n",
    "\n",
    "Because these features are nominal categorical variables (they do not represent an explicit numerical order), `One-Hot Encoding` is applied. This approach prevents the introduction of artificial ordinal relationships that could negatively impact distance-based and linear models.\n",
    "\n",
    "**The following categorical features are encoded using One-Hot Encoding:**\n",
    "\n",
    "- `gender`\n",
    "\n",
    "- `country`\n",
    "\n",
    "- `city`\n",
    "\n",
    "- `customer_segment`\n",
    "\n",
    "- `signup_channel`\n",
    "\n",
    "- `contract_type`\n",
    "\n",
    "- `discount_applied`\n",
    "\n",
    "- `price_increase_last_3m`\n",
    "\n",
    "- `survey_response`\n",
    "\n",
    "Numerical features are left unchanged at this stage and will be processed separately during scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "LORzW48xtU3D",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1769958232792,
     "user": {
      "displayName": "Cristi Turcin",
      "userId": "04700934510619811375"
     },
     "user_tz": -120
    },
    "id": "LORzW48xtU3D",
    "outputId": "0d5dc8b5-d169-461f-e304-9adffdc46961"
   },
   "outputs": [],
   "source": [
    "\n",
    "categorical_cols = many_feat_dataset.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "many_feat_dataset = pd.get_dummies(\n",
    "    many_feat_dataset,\n",
    "    columns=categorical_cols,\n",
    "    drop_first=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5595ac",
   "metadata": {},
   "source": [
    "We will perform the same operation with the `rel_feat_dataset` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "57406ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = rel_feat_dataset.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "rel_feat_dataset = pd.get_dummies(\n",
    "    rel_feat_dataset,\n",
    "    columns=categorical_cols,\n",
    "    drop_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bc2366",
   "metadata": {},
   "source": [
    "And the `h1_feat_dataset`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1dc4cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = hq_feat_dataset.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "hq_feat_dataset = pd.get_dummies(\n",
    "    hq_feat_dataset,\n",
    "    columns=categorical_cols,\n",
    "    drop_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YIkUKEV-p5tp",
   "metadata": {
    "id": "YIkUKEV-p5tp"
   },
   "source": [
    "### 2. Splitting the Dataset\n",
    "\n",
    "The dataset is split into training and test sets using a 70%–30% ratio. Stratified sampling is applied to maintain the original class distribution of the target variable, and a fixed random seed is used to ensure reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "_giHFHh3qEtp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1769958286803,
     "user": {
      "displayName": "Cristi Turcin",
      "userId": "04700934510619811375"
     },
     "user_tz": -120
    },
    "id": "_giHFHh3qEtp",
    "outputId": "ddc47190-6d9e-4b74-8ccd-9aba5225ca47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7000, 42)\n",
      "Test shape: (3000, 42)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features and target\n",
    "X_test_many_feat = many_feat_dataset.drop(columns='churn')\n",
    "y_test_many_feat = many_feat_dataset['churn']\n",
    "\n",
    "# Train / Test split\n",
    "X_train_many_feat, X_test_many_feat, y_train_many_feat, y_test_many_feat = train_test_split(\n",
    "    X_test_many_feat,\n",
    "    y_test_many_feat,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y_test_many_feat\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train_many_feat.shape)\n",
    "print(\"Test shape:\", X_test_many_feat.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743a550a",
   "metadata": {},
   "source": [
    "Let's split the other datasets as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7b138e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7000, 12)\n",
      "Test shape: (3000, 12)\n"
     ]
    }
   ],
   "source": [
    "X_test_rel_feat = rel_feat_dataset.drop(columns='churn')\n",
    "y_test_rel_feat = rel_feat_dataset['churn']\n",
    "\n",
    "X_train_rel_feat, X_test_rel_feat, y_train_rel_feat, y_test_rel_feat = train_test_split(\n",
    "    X_test_rel_feat,\n",
    "    y_test_rel_feat,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y_test_rel_feat\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train_rel_feat.shape)\n",
    "print(\"Test shape:\", X_test_rel_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "610b06fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7000, 2)\n",
      "Test shape: (3000, 2)\n"
     ]
    }
   ],
   "source": [
    "X_test_hq_feat = hq_feat_dataset.drop(columns='churn')\n",
    "y_test_hq_feat = hq_feat_dataset['churn']\n",
    "\n",
    "X_train_hq_feat, X_test_hq_feat, y_train_hq_feat, y_test_hq_feat = train_test_split(\n",
    "    X_test_hq_feat,\n",
    "    y_test_hq_feat,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y_test_hq_feat\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train_hq_feat.shape)\n",
    "print(\"Test shape:\", X_test_hq_feat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zbI570SiqHje",
   "metadata": {
    "id": "zbI570SiqHje"
   },
   "source": [
    "### 3. Scaling\n",
    "We will use the standard scaler in order to transform the data into feedable entries. The standard scaler is picked becouse it works gread for classification models such as __SVM__ and __Logistic Regression__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "DAJD__Y8qJ2u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1769958290601,
     "user": {
      "displayName": "Cristi Turcin",
      "userId": "04700934510619811375"
     },
     "user_tz": -120
    },
    "id": "DAJD__Y8qJ2u",
    "outputId": "763fbb46-0b2b-4da2-9d64-19b2908d9e26"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>tenure_months</th>\n",
       "      <th>monthly_logins</th>\n",
       "      <th>weekly_active_days</th>\n",
       "      <th>avg_session_time</th>\n",
       "      <th>features_used</th>\n",
       "      <th>usage_growth_rate</th>\n",
       "      <th>last_login_days_ago</th>\n",
       "      <th>monthly_fee</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>...</th>\n",
       "      <th>signup_channel_Referral</th>\n",
       "      <th>signup_channel_Web</th>\n",
       "      <th>contract_type_Quarterly</th>\n",
       "      <th>contract_type_Yearly</th>\n",
       "      <th>payment_method_Card</th>\n",
       "      <th>payment_method_PayPal</th>\n",
       "      <th>discount_applied_Yes</th>\n",
       "      <th>price_increase_last_3m_Yes</th>\n",
       "      <th>survey_response_Satisfied</th>\n",
       "      <th>survey_response_Unsatisfied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>-1.576243</td>\n",
       "      <td>-0.072337</td>\n",
       "      <td>1.050666</td>\n",
       "      <td>-1.511833</td>\n",
       "      <td>0.797427</td>\n",
       "      <td>0.898253</td>\n",
       "      <td>-0.272831</td>\n",
       "      <td>-0.560739</td>\n",
       "      <td>-0.209509</td>\n",
       "      <td>-0.185256</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6331</th>\n",
       "      <td>1.703909</td>\n",
       "      <td>-1.358107</td>\n",
       "      <td>-0.379765</td>\n",
       "      <td>-1.078377</td>\n",
       "      <td>-0.090936</td>\n",
       "      <td>-1.345296</td>\n",
       "      <td>-1.471208</td>\n",
       "      <td>-0.764649</td>\n",
       "      <td>-0.628048</td>\n",
       "      <td>-0.897613</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6323</th>\n",
       "      <td>-0.118398</td>\n",
       "      <td>0.512104</td>\n",
       "      <td>-2.014544</td>\n",
       "      <td>0.221992</td>\n",
       "      <td>0.372326</td>\n",
       "      <td>2.244382</td>\n",
       "      <td>-0.006524</td>\n",
       "      <td>1.172499</td>\n",
       "      <td>-0.628048</td>\n",
       "      <td>-0.273081</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7739</th>\n",
       "      <td>-0.786577</td>\n",
       "      <td>-0.189225</td>\n",
       "      <td>-2.014544</td>\n",
       "      <td>-1.078377</td>\n",
       "      <td>1.200962</td>\n",
       "      <td>-0.447876</td>\n",
       "      <td>-1.338055</td>\n",
       "      <td>-0.152918</td>\n",
       "      <td>0.627570</td>\n",
       "      <td>0.283143</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>1.217960</td>\n",
       "      <td>-0.013893</td>\n",
       "      <td>1.459361</td>\n",
       "      <td>-0.211465</td>\n",
       "      <td>-0.360284</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>1.058700</td>\n",
       "      <td>-0.050963</td>\n",
       "      <td>-0.628048</td>\n",
       "      <td>-0.448731</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           age  tenure_months  monthly_logins  weekly_active_days  \\\n",
       "1078 -1.576243      -0.072337        1.050666           -1.511833   \n",
       "6331  1.703909      -1.358107       -0.379765           -1.078377   \n",
       "6323 -0.118398       0.512104       -2.014544            0.221992   \n",
       "7739 -0.786577      -0.189225       -2.014544           -1.078377   \n",
       "547   1.217960      -0.013893        1.459361           -0.211465   \n",
       "\n",
       "      avg_session_time  features_used  usage_growth_rate  last_login_days_ago  \\\n",
       "1078          0.797427       0.898253          -0.272831            -0.560739   \n",
       "6331         -0.090936      -1.345296          -1.471208            -0.764649   \n",
       "6323          0.372326       2.244382          -0.006524             1.172499   \n",
       "7739          1.200962      -0.447876          -1.338055            -0.152918   \n",
       "547          -0.360284       0.000833           1.058700            -0.050963   \n",
       "\n",
       "      monthly_fee  total_revenue  ...  signup_channel_Referral  \\\n",
       "1078    -0.209509      -0.185256  ...                    False   \n",
       "6331    -0.628048      -0.897613  ...                    False   \n",
       "6323    -0.628048      -0.273081  ...                    False   \n",
       "7739     0.627570       0.283143  ...                    False   \n",
       "547     -0.628048      -0.448731  ...                    False   \n",
       "\n",
       "      signup_channel_Web  contract_type_Quarterly  contract_type_Yearly  \\\n",
       "1078                True                     True                 False   \n",
       "6331               False                     True                 False   \n",
       "6323                True                    False                  True   \n",
       "7739               False                    False                  True   \n",
       "547                 True                    False                 False   \n",
       "\n",
       "      payment_method_Card  payment_method_PayPal  discount_applied_Yes  \\\n",
       "1078                 True                  False                 False   \n",
       "6331                 True                  False                 False   \n",
       "6323                False                  False                 False   \n",
       "7739                False                  False                  True   \n",
       "547                 False                   True                 False   \n",
       "\n",
       "      price_increase_last_3m_Yes  survey_response_Satisfied  \\\n",
       "1078                       False                      False   \n",
       "6331                        True                       True   \n",
       "6323                        True                      False   \n",
       "7739                        True                       True   \n",
       "547                        False                       True   \n",
       "\n",
       "      survey_response_Unsatisfied  \n",
       "1078                         True  \n",
       "6331                        False  \n",
       "6323                        False  \n",
       "7739                        False  \n",
       "547                         False  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select numerical columns\n",
    "numerical_cols = X_train_many_feat.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data only\n",
    "X_train_many_feat[numerical_cols] = scaler.fit_transform(X_train_many_feat[numerical_cols])\n",
    "X_test_many_feat[numerical_cols] = scaler.transform(X_test_many_feat[numerical_cols])\n",
    "\n",
    "X_train_many_feat.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392d4687",
   "metadata": {},
   "source": [
    "We will scale the other datasets, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e47c673e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>monthly_logins</th>\n",
       "      <th>weekly_active_days</th>\n",
       "      <th>usage_growth_rate</th>\n",
       "      <th>payment_failures</th>\n",
       "      <th>support_tickets</th>\n",
       "      <th>avg_resolution_time</th>\n",
       "      <th>csat_score</th>\n",
       "      <th>escalations</th>\n",
       "      <th>nps_score</th>\n",
       "      <th>referral_count</th>\n",
       "      <th>survey_response_Satisfied</th>\n",
       "      <th>survey_response_Unsatisfied</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>1.050666</td>\n",
       "      <td>-1.511833</td>\n",
       "      <td>-0.272831</td>\n",
       "      <td>-0.701872</td>\n",
       "      <td>-0.192313</td>\n",
       "      <td>-0.931100</td>\n",
       "      <td>1.558244</td>\n",
       "      <td>1.306013</td>\n",
       "      <td>-0.512638</td>\n",
       "      <td>1.022136</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6331</th>\n",
       "      <td>-0.379765</td>\n",
       "      <td>-1.078377</td>\n",
       "      <td>-1.471208</td>\n",
       "      <td>-0.701872</td>\n",
       "      <td>1.620742</td>\n",
       "      <td>-1.299269</td>\n",
       "      <td>-1.516659</td>\n",
       "      <td>-0.535665</td>\n",
       "      <td>0.180394</td>\n",
       "      <td>-0.989943</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6323</th>\n",
       "      <td>-2.014544</td>\n",
       "      <td>0.221992</td>\n",
       "      <td>-0.006524</td>\n",
       "      <td>0.709129</td>\n",
       "      <td>-0.192313</td>\n",
       "      <td>-0.367868</td>\n",
       "      <td>-0.491692</td>\n",
       "      <td>1.306013</td>\n",
       "      <td>2.002810</td>\n",
       "      <td>-0.989943</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7739</th>\n",
       "      <td>-2.014544</td>\n",
       "      <td>-1.078377</td>\n",
       "      <td>-1.338055</td>\n",
       "      <td>-0.701872</td>\n",
       "      <td>0.714214</td>\n",
       "      <td>-0.520712</td>\n",
       "      <td>1.558244</td>\n",
       "      <td>-0.535665</td>\n",
       "      <td>-0.230292</td>\n",
       "      <td>-0.989943</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>1.459361</td>\n",
       "      <td>-0.211465</td>\n",
       "      <td>1.058700</td>\n",
       "      <td>-0.701872</td>\n",
       "      <td>-1.098841</td>\n",
       "      <td>-0.854083</td>\n",
       "      <td>1.558244</td>\n",
       "      <td>1.306013</td>\n",
       "      <td>-1.282673</td>\n",
       "      <td>0.016097</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      monthly_logins  weekly_active_days  usage_growth_rate  payment_failures  \\\n",
       "1078        1.050666           -1.511833          -0.272831         -0.701872   \n",
       "6331       -0.379765           -1.078377          -1.471208         -0.701872   \n",
       "6323       -2.014544            0.221992          -0.006524          0.709129   \n",
       "7739       -2.014544           -1.078377          -1.338055         -0.701872   \n",
       "547         1.459361           -0.211465           1.058700         -0.701872   \n",
       "\n",
       "      support_tickets  avg_resolution_time  csat_score  escalations  \\\n",
       "1078        -0.192313            -0.931100    1.558244     1.306013   \n",
       "6331         1.620742            -1.299269   -1.516659    -0.535665   \n",
       "6323        -0.192313            -0.367868   -0.491692     1.306013   \n",
       "7739         0.714214            -0.520712    1.558244    -0.535665   \n",
       "547         -1.098841            -0.854083    1.558244     1.306013   \n",
       "\n",
       "      nps_score  referral_count  survey_response_Satisfied  \\\n",
       "1078  -0.512638        1.022136                      False   \n",
       "6331   0.180394       -0.989943                       True   \n",
       "6323   2.002810       -0.989943                      False   \n",
       "7739  -0.230292       -0.989943                       True   \n",
       "547   -1.282673        0.016097                       True   \n",
       "\n",
       "      survey_response_Unsatisfied  \n",
       "1078                         True  \n",
       "6331                        False  \n",
       "6323                        False  \n",
       "7739                        False  \n",
       "547                         False  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_cols = X_train_rel_feat.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_rel_feat[numerical_cols] = scaler.fit_transform(X_train_rel_feat[numerical_cols])\n",
    "X_test_rel_feat[numerical_cols] = scaler.transform(X_test_rel_feat[numerical_cols])\n",
    "\n",
    "X_train_rel_feat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8814e700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>csat_score</th>\n",
       "      <th>nps_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>1.558244</td>\n",
       "      <td>-0.512638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6331</th>\n",
       "      <td>-1.516659</td>\n",
       "      <td>0.180394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6323</th>\n",
       "      <td>-0.491692</td>\n",
       "      <td>2.002810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7739</th>\n",
       "      <td>1.558244</td>\n",
       "      <td>-0.230292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>1.558244</td>\n",
       "      <td>-1.282673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      csat_score  nps_score\n",
       "1078    1.558244  -0.512638\n",
       "6331   -1.516659   0.180394\n",
       "6323   -0.491692   2.002810\n",
       "7739    1.558244  -0.230292\n",
       "547     1.558244  -1.282673"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_cols = X_train_hq_feat.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_hq_feat[numerical_cols] = scaler.fit_transform(X_train_hq_feat[numerical_cols])\n",
    "X_test_hq_feat[numerical_cols] = scaler.transform(X_test_hq_feat[numerical_cols])\n",
    "\n",
    "X_train_hq_feat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878696cc",
   "metadata": {
    "id": "878696cc"
   },
   "source": [
    "### 4. Dimensionality reduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a333fb16",
   "metadata": {
    "id": "a333fb16"
   },
   "source": [
    "PCA with three components is applied after scaling. The transformation is fitted on the training data only to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "b6b7c55a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 186,
     "status": "ok",
     "timestamp": 1769958293354,
     "user": {
      "displayName": "Cristi Turcin",
      "userId": "04700934510619811375"
     },
     "user_tz": -120
    },
    "id": "b6b7c55a",
    "outputId": "3e7262b5-8abb-4bd9-c113-3363f7446717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original feature space: (7000, 42)\n",
      "Reduced feature space: (7000, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA with 3 components\n",
    "pca = PCA(n_components=3, random_state=42)\n",
    "\n",
    "# Fit PCA on training data only\n",
    "X_train_pca = pca.fit_transform(X_train_many_feat)\n",
    "X_test_pca = pca.transform(X_test_many_feat)\n",
    "\n",
    "print(\"Original feature space:\", X_train_many_feat.shape)\n",
    "print(\"Reduced feature space:\", X_train_pca.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c613c66d",
   "metadata": {
    "id": "c613c66d"
   },
   "source": [
    "## IV. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb32cf",
   "metadata": {
    "id": "6bcb32cf"
   },
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b17abedc",
   "metadata": {
    "executionInfo": {
     "elapsed": 85,
     "status": "ok",
     "timestamp": 1769958354425,
     "user": {
      "displayName": "Cristi Turcin",
      "userId": "04700934510619811375"
     },
     "user_tz": -120
    },
    "id": "b17abedc"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model_many = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000).fit(X_train_many_feat, y_train_many_feat)\n",
    "\n",
    "lr_model_rel = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000).fit(X_train_rel_feat, y_train_rel_feat)\n",
    "\n",
    "lr_model_hq = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000).fit(X_train_hq_feat, y_train_hq_feat)\n",
    "\n",
    "lr_model_pca = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000).fit(X_train_pca, y_train_many_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c580a2",
   "metadata": {
    "id": "26c580a2"
   },
   "source": [
    "### 2. Support Vector Machines (SVMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4d97cfa5",
   "metadata": {
    "executionInfo": {
     "elapsed": 22283,
     "status": "ok",
     "timestamp": 1769958449853,
     "user": {
      "displayName": "Cristi Turcin",
      "userId": "04700934510619811375"
     },
     "user_tz": -120
    },
    "id": "4d97cfa5"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model_many = SVC(class_weight='balanced', kernel='rbf', C=0.1, gamma='scale', random_state=42).fit(X_train_many_feat, y_train_many_feat)\n",
    "\n",
    "svc_model_rel = SVC(class_weight='balanced', kernel='rbf', C=0.1, gamma='scale', random_state=42).fit(X_train_rel_feat, y_train_rel_feat)\n",
    "\n",
    "svc_model_hq = SVC(class_weight='balanced', kernel='rbf', C=0.1, gamma='scale', random_state=42).fit(X_train_hq_feat, y_train_hq_feat)\n",
    "\n",
    "svc_model_pca = SVC(class_weight='balanced', kernel='rbf', C=0.1, gamma='scale', random_state=42).fit(X_train_pca, y_train_many_feat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04913d0b",
   "metadata": {
    "id": "04913d0b"
   },
   "source": [
    "### 3. Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wAGG1c7HwEgQ",
   "metadata": {
    "id": "wAGG1c7HwEgQ"
   },
   "source": [
    "Decision Tree classifiers are trained using the Gini impurity criterion. Although tree-based models do not require feature scaling or dimensionality reduction, the model is also trained on the PCA-reduced dataset for comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "08fd600a",
   "metadata": {
    "executionInfo": {
     "elapsed": 601,
     "status": "ok",
     "timestamp": 1769958586667,
     "user": {
      "displayName": "Cristi Turcin",
      "userId": "04700934510619811375"
     },
     "user_tz": -120
    },
    "id": "08fd600a"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dct_clf_many = DecisionTreeClassifier(class_weight='balanced', criterion='gini', random_state=42).fit(X_train_many_feat, y_train_many_feat)\n",
    "\n",
    "dct_clf_rel = DecisionTreeClassifier(class_weight='balanced', criterion='gini', random_state=42).fit(X_train_rel_feat, y_train_rel_feat)\n",
    "\n",
    "dct_clf_hq = DecisionTreeClassifier(class_weight='balanced', criterion='gini', random_state=42).fit(X_train_hq_feat, y_train_hq_feat)\n",
    "\n",
    "dct_clf_pca = DecisionTreeClassifier(class_weight='balanced', criterion='gini', random_state=42).fit(X_train_pca, y_train_many_feat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb3eb5e",
   "metadata": {
    "id": "3bb3eb5e"
   },
   "source": [
    "## V. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce7f850",
   "metadata": {
    "id": "dce7f850"
   },
   "source": [
    "### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d3257089",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1769958735463,
     "user": {
      "displayName": "Cristi Turcin",
      "userId": "04700934510619811375"
     },
     "user_tz": -120
    },
    "id": "d3257089",
    "outputId": "9ebea8ad-bcc2-47b7-f235-82a072b36f37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for Logistic Regression using all features: 0.6753333333333333\n",
      "F1 score for Logistic Regression using all features: 0.2859237536656892 \n",
      "\n",
      "Accuracy score for Logistic Regression using relevant features: 0.654\n",
      "F1 score for Logistic Regression using relevant features: 0.26487252124645894 \n",
      "\n",
      "Accuracy score for Logistic Regression using HQ features: 0.5403333333333333\n",
      "F1 score for Logistic Regression using HQ features: 0.2151394422310757 \n",
      "\n",
      "Accuracy score for Logistic Regression using all features: 0.5443333333333333\n",
      "F1 score for Logistic Regression using reduced features: 0.20845396641574984\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "y_pred_lr_many = lr_model_many.predict(X_test_many_feat)\n",
    "results_lr_many_acc = accuracy_score(y_test_many_feat, y_pred_lr_many)\n",
    "results_lr_many_f1 = f1_score(y_test_many_feat, y_pred_lr_many)\n",
    "\n",
    "y_pred_lr_rel = lr_model_rel.predict(X_test_rel_feat)\n",
    "results_lr_rel_acc = accuracy_score(y_test_rel_feat, y_pred_lr_rel)\n",
    "results_lr_rel_f1 = f1_score(y_test_rel_feat, y_pred_lr_rel)\n",
    "\n",
    "y_pred_lr_hq = lr_model_hq.predict(X_test_hq_feat)\n",
    "results_lr_hq_acc = accuracy_score(y_test_hq_feat, y_pred_lr_hq)\n",
    "results_lr_hq_f1 = f1_score(y_test_hq_feat, y_pred_lr_hq)   \n",
    "\n",
    "y_pred_lr_pca = lr_model_pca.predict(X_test_pca)\n",
    "results_lr_pca_acc = accuracy_score(y_test_many_feat, y_pred_lr_pca)\n",
    "results_lr_pca_f1 = f1_score(y_test_many_feat, y_pred_lr_pca)\n",
    "\n",
    "print(\"Accuracy score for Logistic Regression using all features:\", results_lr_many_acc)\n",
    "print(\"F1 score for Logistic Regression using all features:\", results_lr_many_f1, \"\\n\")\n",
    "\n",
    "print(\"Accuracy score for Logistic Regression using relevant features:\", results_lr_rel_acc)\n",
    "print(\"F1 score for Logistic Regression using relevant features:\", results_lr_rel_f1, \"\\n\")\n",
    "\n",
    "print(\"Accuracy score for Logistic Regression using HQ features:\", results_lr_hq_acc)\n",
    "print(\"F1 score for Logistic Regression using HQ features:\", results_lr_hq_f1, \"\\n\")\n",
    "\n",
    "print(\"Accuracy score for Logistic Regression using all features:\", results_lr_pca_acc)\n",
    "print(\"F1 score for Logistic Regression using reduced features:\", results_lr_pca_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e8164e",
   "metadata": {
    "id": "e5e8164e"
   },
   "source": [
    "### 2. Suppor Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1c34b78d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2804,
     "status": "ok",
     "timestamp": 1769958742188,
     "user": {
      "displayName": "Cristi Turcin",
      "userId": "04700934510619811375"
     },
     "user_tz": -120
    },
    "id": "1c34b78d",
    "outputId": "3e1e4c1f-9306-4a16-d09f-95d46a2ff8d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for SVM using all features: 0.6876666666666666\n",
      "F1 score for SVM using all features: 0.3234657039711191 \n",
      "\n",
      "Accuracy score for SVM using relevant features: 0.697\n",
      "F1 score for SVM using relevant features: 0.3045141545524101 \n",
      "\n",
      "Accuracy score for SVM using HQ features: 0.8163333333333334\n",
      "F1 score for SVM using HQ features: 0.2778505897771953 \n",
      "\n",
      "Accuracy score for SVM using reduced features: 0.6243333333333333\n",
      "F1 score for SVM using reduced features: 0.23281143635125937\n"
     ]
    }
   ],
   "source": [
    "y_pred_svc_many = svc_model_many.predict(X_test_many_feat)\n",
    "results_svc_many_acc = accuracy_score(y_test_many_feat, y_pred_svc_many)\n",
    "results_svc_many_f1 = f1_score(y_test_many_feat, y_pred_svc_many)\n",
    "\n",
    "y_pred_svc_rel = svc_model_rel.predict(X_test_rel_feat)\n",
    "results_svc_rel_acc = accuracy_score(y_test_rel_feat, y_pred_svc_rel)\n",
    "results_svc_rel_f1 = f1_score(y_test_rel_feat, y_pred_svc_rel)\n",
    "\n",
    "y_pred_svc_hq = svc_model_hq.predict(X_test_hq_feat)\n",
    "results_svc_hq_acc = accuracy_score(y_test_hq_feat, y_pred_svc_hq)\n",
    "results_svc_hq_f1 = f1_score(y_test_hq_feat, y_pred_svc_hq)\n",
    "\n",
    "y_pred_svc_pca = svc_model_pca.predict(X_test_pca)\n",
    "results_svc_pca_acc = accuracy_score(y_test_many_feat, y_pred_svc_pca)\n",
    "results_svc_pca_f1 = f1_score(y_test_many_feat, y_pred_svc_pca)\n",
    "\n",
    "print(\"Accuracy score for SVM using all features:\", results_svc_many_acc)\n",
    "print(\"F1 score for SVM using all features:\", results_svc_many_f1, \"\\n\")\n",
    "\n",
    "print(\"Accuracy score for SVM using relevant features:\", results_svc_rel_acc)\n",
    "print(\"F1 score for SVM using relevant features:\", results_svc_rel_f1, \"\\n\")\n",
    "\n",
    "print(\"Accuracy score for SVM using HQ features:\", results_svc_hq_acc)\n",
    "print(\"F1 score for SVM using HQ features:\", results_svc_hq_f1, \"\\n\")\n",
    "\n",
    "print(\"Accuracy score for SVM using reduced features:\", results_svc_pca_acc)\n",
    "print(\"F1 score for SVM using reduced features:\", results_svc_pca_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee929acb",
   "metadata": {
    "id": "ee929acb"
   },
   "source": [
    "### 3. Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "26fd656b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1769958744852,
     "user": {
      "displayName": "Cristi Turcin",
      "userId": "04700934510619811375"
     },
     "user_tz": -120
    },
    "id": "26fd656b",
    "outputId": "75011273-fef6-4b17-8e10-a9afdcf61360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score for Decision Tree using all features: 0.8293333333333334\n",
      "F1 score for Decision Tree using all features: 0.16339869281045752 \n",
      "\n",
      "Accuracy score for Decision Tree using relevant features: 0.8423333333333334\n",
      "F1 score for Decision Tree using relevant features: 0.21818181818181817 \n",
      "\n",
      "Accuracy score for Decision Tree using HQ features: 0.6526666666666666\n",
      "F1 score for Decision Tree using HQ features: 0.18973561430793157 \n",
      "\n",
      "Accuracy score for Decision Tree using reduced features: 0.81\n",
      "F1 score for Decision Tree using reduced features: 0.13636363636363635\n"
     ]
    }
   ],
   "source": [
    "y_pred_dt_many = dct_clf_many.predict(X_test_many_feat)\n",
    "results_dt_many_acc = accuracy_score(y_test_many_feat, y_pred_dt_many)\n",
    "results_dt_many = f1_score(y_test_many_feat, y_pred_dt_many)\n",
    "\n",
    "y_pred_dt_rel = dct_clf_rel.predict(X_test_rel_feat)\n",
    "results_dt_rel_acc = accuracy_score(y_test_rel_feat, y_pred_dt_rel)\n",
    "results_dt_rel = f1_score(y_test_rel_feat, y_pred_dt_rel)\n",
    "\n",
    "y_pred_dt_hq = dct_clf_hq.predict(X_test_hq_feat)\n",
    "results_dt_hq_acc = accuracy_score(y_test_hq_feat, y_pred_dt_hq)\n",
    "results_dt_hq = f1_score(y_test_hq_feat, y_pred_dt_hq)\n",
    "\n",
    "y_pred_dt_pca = dct_clf_pca.predict(X_test_pca)\n",
    "results_dt_pca_acc = accuracy_score(y_test_many_feat, y_pred_dt_pca)\n",
    "results_dt_pca = f1_score(y_test_many_feat, y_pred_dt_pca)\n",
    "\n",
    "print(\"Accuracy score for Decision Tree using all features:\", results_dt_many_acc)\n",
    "print(\"F1 score for Decision Tree using all features:\", results_dt_many, \"\\n\")\n",
    "\n",
    "print(\"Accuracy score for Decision Tree using relevant features:\", results_dt_rel_acc)\n",
    "print(\"F1 score for Decision Tree using relevant features:\", results_dt_rel, \"\\n\")\n",
    "\n",
    "print(\"Accuracy score for Decision Tree using HQ features:\", results_dt_hq_acc)\n",
    "print(\"F1 score for Decision Tree using HQ features:\", results_dt_hq, \"\\n\")\n",
    "\n",
    "print(\"Accuracy score for Decision Tree using reduced features:\", results_dt_pca_acc)\n",
    "print(\"F1 score for Decision Tree using reduced features:\", results_dt_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54880c9",
   "metadata": {
    "id": "a54880c9"
   },
   "source": [
    "## VI. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09e6886",
   "metadata": {
    "id": "a09e6886"
   },
   "source": [
    "As a result of this experiment, the following conclusions can be drawn:\n",
    "\n",
    "• The dataset is highly imbalanced, with significantly fewer churned customers compared to non-churned ones, which makes the classification task more challenging and justifies the use of the F1-score as an evaluation metric.\n",
    "\n",
    "• Among the evaluated models, Support Vector Machines achieved the best performance when trained on the full feature set, indicating their ability to capture complex, non-linear relationships in the data.\n",
    "\n",
    "• Applying Principal Component Analysis (PCA) consistently reduced model performance across all algorithms, suggesting that dimensionality reduction led to the loss of important predictive information.\n",
    "\n",
    "• Overall, models trained on the original feature space performed better than those trained on the PCA-reduced data, highlighting the importance of preserving the full set of engineered features for churn prediction.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
